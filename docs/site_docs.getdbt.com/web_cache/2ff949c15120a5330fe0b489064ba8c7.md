# seeds/my_seed.csv  
my_seed_csv = """  
id,name,some_date  
1,Easton,1981-05-20T06:46:51  
2,Lillian,1978-09-03T18:10:33  
3,Jeremiah,1982-03-11T03:59:51  
4,Nolan,1976-05-06T20:21:35  
""".lstrip()  
  
# models/my_model.sql  
my_model_sql = """  
select * from {{ ref('my_seed') }}  
union all  
select null as id, null as name, null as some_date  
"""  
  
# models/my_model.yml  
my_model_yml = """  
version: 2  
models:  
  - name: my_model  
    columns:  
      - name: id  
        data_tests:  
          - unique  
          - not_null  # this test will fail  
"""
```

2. Use the "fixtures" to define the project for your test case. These fixtures are always scoped to the **class**, where the class represents one test case—that is, one dbt project or scenario. (The same test case can be used for one or more actual tests, which we'll see in step 3.) Following the default pytest configurations, the file name must begin with `test_`, and the class name must begin with `Test`.

tests/functional/example/test\_example\_failing\_test.py

```
import pytest  
from dbt.tests.util import run_dbt  
  
# our file contents  
from tests.functional.example.fixtures import (  
    my_seed_csv,  
    my_model_sql,  
    my_model_yml,  
)  
  
# class must begin with 'Test'  
class TestExample:  
    """  
    Methods in this class will be of two types:  
    1. Fixtures defining the dbt "project" for this test case.  
       These are scoped to the class, and reused for all tests in the class.  
    2. Actual tests, whose names begin with 'test_'.  
       These define sequences of dbt commands and 'assert' statements.  
    """  
      
    # configuration in dbt_project.yml  
    @pytest.fixture(scope="class")  
    def project_config_update(self):  
        return {  
          "name": "example",  
          "models": {"+materialized": "view"}  
        }  
  
    # everything that goes in the "seeds" directory  
    @pytest.fixture(scope="class")  
    def seeds(self):  
        return {  
            "my_seed.csv": my_seed_csv,  
        }  
  
    # everything that goes in the "models" directory  
    @pytest.fixture(scope="class")  
    def models(self):  
        return {  
            "my_model.sql": my_model_sql,  
            "my_model.yml": my_model_yml,  
        }  
          
    # continues below
```

3. Now that we've set up our project, it's time to define a sequence of dbt commands and assertions. We define one or more methods in the same file, on the same class (`TestExampleFailingTest`), whose names begin with `test_`. These methods share the same setup (project scenario) from above, but they can be run independently by pytest—so they shouldn't depend on each other in any way.

tests/functional/example/test\_example\_failing\_test.py

```
    # continued from above  
  
    # The actual sequence of dbt commands and assertions  
    # pytest will take care of all "setup" + "teardown"  
    def test_run_seed_test(self, project):  
        """  
        Seed, then run, then test. We expect one of the tests to fail  
        An alternative pattern is to use pytest "xfail" (see below)  
        """  
        # seed seeds  
        results = run_dbt(["seed"])  
        assert len(results) == 1  
        # run models  
        results = run_dbt(["run"])  
        assert len(results) == 1  
        # test tests  
        results = run_dbt(["test"], expect_pass = False) # expect failing test  
        assert len(results) == 2  
        # validate that the results include one pass and one failure  
        result_statuses = sorted(r.status for r in results)  
        assert result_statuses == ["fail", "pass"]  
  
    @pytest.mark.xfail  
    def test_build(self, project):  
        """Expect a failing test"""  
        # do it all  
        results = run_dbt(["build"])
```

3. Our test is ready to run! The last step is to invoke `pytest` from your command line. We'll walk through the actual setup and configuration of `pytest` in the next section.

terminal

```
$ python3 -m pytest tests/functional/test_example.py  
=========================== test session starts ============================  
platform ... -- Python ..., pytest-..., pluggy-...  
rootdir: ...  
plugins: ...  
  
tests/functional/test_example.py .X                                  [100%]  
  
======================= 1 passed, 1 xpassed in 1.38s =======================
```

You can find more ways to run tests, along with a full command reference, in the [pytest usage docs](https://docs.pytest.org/how-to/usage.html).

We've found the `-s` flag (or `--capture=no`) helpful to print logs from the underlying dbt invocations, and to step into an interactive debugger if you've added one. You can also use environment variables to set [global dbt configs](/reference/global-configs/about-global-configs), such as `DBT_DEBUG` (to show debug-level logs).

### Testing this adapter[​](#testing-this-adapter "Direct link to Testing this adapter")

Anyone who installs `dbt-core`, and wishes to define their own test cases, can use the framework presented in the first section. The framework is especially useful for testing standard dbt behavior across different databases.

To that end, we have built and made available a [package of reusable adapter test cases](https://github.com/dbt-labs/dbt-adapters/tree/main/dbt-tests-adapter), for creators and maintainers of adapter plugins. These test cases cover basic expected functionality, as well as functionality that frequently requires different implementations across databases.

For the time being, this package is also located within the `dbt-core` repository, but separate from the `dbt-core` Python package.

### Categories of tests[​](#categories-of-tests "Direct link to Categories of tests")

In the course of creating and maintaining your adapter, it's likely that you will end up implementing tests that fall into three broad categories:

1. **Basic tests** that every adapter plugin is expected to pass. These are defined in `tests.adapter.basic`. Given differences across data platforms, these may require slight modification or reimplementation. Significantly overriding or disabling these tests should be with good reason, since each represents basic functionality expected by dbt users. For example, if your adapter does not support incremental models, you should disable the test, [by marking it with `skip` or `xfail`](https://docs.pytest.org/en/latest/how-to/skipping.html), as well as noting that limitation in any documentation, READMEs, and usage guides that accompany your adapter.
2. **Optional tests**, for second-order functionality that is common across plugins, but not required for basic use. Your plugin can opt into these test cases by inheriting existing ones, or reimplementing them with adjustments. For now, this category includes all tests located outside the `basic` subdirectory. More tests will be added as we convert older tests defined on dbt-core and mature plugins to use the standard framework.
3. **Custom tests**, for behavior that is specific to your adapter / data platform. Each data warehouse has its own specialties and idiosyncracies. We encourage you to use the same `pytest`-based framework, utilities, and fixtures to write your own custom tests for functionality that is unique to your adapter.

If you run into an issue with the core framework, or the basic/optional test cases—or if you've written a custom test that you believe would be relevant and useful for other adapter plugin developers—please open an issue or PR in the `dbt-core` repository on GitHub.

### Getting started running basic tests[​](#getting-started-running-basic-tests "Direct link to Getting started running basic tests")

In this section, we'll walk through the three steps to start running our basic test cases on your adapter plugin:

1. Install dependencies
2. Set up and configure pytest
3. Define test cases

### Install dependencies[​](#install-dependencies "Direct link to Install dependencies")

You should already have a virtual environment with `dbt-core` and your adapter plugin installed. You'll also need to install:

* [`pytest`](https://pypi.org/project/pytest/)
* [`dbt-tests-adapter`](https://pypi.org/project/dbt-tests-adapter/), the set of common test cases
* (optional) [`pytest` plugins](https://docs.pytest.org/en/7.0.x/reference/plugin_list.html)--we'll use `pytest-dotenv` below

Or specify all dependencies in a requirements file like:

dev\_requirements.txt

```
pytest  
pytest-dotenv  
dbt-tests-adapter
```

```
python -m pip install -r dev_requirements.txt
```

### Set up and configure pytest[​](#set-up-and-configure-pytest "Direct link to Set up and configure pytest")

First, set yourself up to run `pytest` by creating a file named `pytest.ini` at the root of your repository:

pytest.ini

```
[pytest]  
filterwarnings =  
    ignore:.*'soft_unicode' has been renamed to 'soft_str'*:DeprecationWarning  
    ignore:unclosed file .*:ResourceWarning  
env_files =  
    test.env  # uses pytest-dotenv plugin  
              # this allows you to store env vars for database connection in a file named test.env  
              # rather than passing them in every CLI command, or setting in `PYTEST_ADDOPTS`  
              # be sure to add "test.env" to .gitignore as well!  
testpaths =  
    tests/functional  # name per convention
```

Then, create a configuration file within your tests directory. In it, you'll want to define all necessary profile configuration for connecting to your data platform in local development and continuous integration. We recommend setting these values with environment variables, since this file will be checked into version control.

tests/conftest.py

```
import pytest  
import os  
  
# Import the standard functional fixtures as a plugin  
# Note: fixtures with session scope need to be local  
pytest_plugins = ["dbt.tests.fixtures.project"]  
  
# The profile dictionary, used to write out profiles.yml  
# dbt will supply a unique schema per test, so we do not specify 'schema' here  
@pytest.fixture(scope="class")  
def dbt_profile_target():  
    return {  
        'type': '<myadapter>',  
        'threads': 1,  
        'host': os.getenv('HOST_ENV_VAR_NAME'),  
        'user': os.getenv('USER_ENV_VAR_NAME'),  
        ...  
    }
```

### Define test cases[​](#define-test-cases "Direct link to Define test cases")

As in the example above, each test case is defined as a class, and has its own "project" setup. To get started, you can import all basic test cases and try running them without changes.

tests/functional/adapter/test\_basic.py

```
import pytest  
  
from dbt.tests.adapter.basic.test_base import BaseSimpleMaterializations  
from dbt.tests.adapter.basic.test_singular_tests import BaseSingularTests  
from dbt.tests.adapter.basic.test_singular_tests_ephemeral import BaseSingularTestsEphemeral  
from dbt.tests.adapter.basic.test_empty import BaseEmpty  
from dbt.tests.adapter.basic.test_ephemeral import BaseEphemeral  
from dbt.tests.adapter.basic.test_incremental import BaseIncremental  
from dbt.tests.adapter.basic.test_generic_tests import BaseGenericTests  
from dbt.tests.adapter.basic.test_snapshot_check_cols import BaseSnapshotCheckCols  
from dbt.tests.adapter.basic.test_snapshot_timestamp import BaseSnapshotTimestamp  
from dbt.tests.adapter.basic.test_adapter_methods import BaseAdapterMethod  
  
class TestSimpleMaterializationsMyAdapter(BaseSimpleMaterializations):  
    pass  
  
  
class TestSingularTestsMyAdapter(BaseSingularTests):  
    pass  
  
  
class TestSingularTestsEphemeralMyAdapter(BaseSingularTestsEphemeral):  
    pass  
  
  
class TestEmptyMyAdapter(BaseEmpty):  
    pass  
  
  
class TestEphemeralMyAdapter(BaseEphemeral):  
    pass  
  
  
class TestIncrementalMyAdapter(BaseIncremental):  
    pass  
  
  
class TestGenericTestsMyAdapter(BaseGenericTests):  
    pass  
  
  
class TestSnapshotCheckColsMyAdapter(BaseSnapshotCheckCols):  
    pass  
  
  
class TestSnapshotTimestampMyAdapter(BaseSnapshotTimestamp):  
    pass  
  
  
class TestBaseAdapterMethod(BaseAdapterMethod):  
    pass
```

Finally, run pytest:

```
python3 -m pytest tests/functional
```

### Modifying test cases[​](#modifying-test-cases "Direct link to Modifying test cases")

You may need to make slight modifications in a specific test case to get it passing on your adapter. The mechanism to do this is simple: rather than simply inheriting the "base" test with `pass`, you can redefine any of its fixtures or test methods.

For instance, on Redshift, we need to explicitly cast a column in the fixture input seed to use data type `varchar(64)`:

tests/functional/adapter/test\_basic.py

```
import pytest  
from dbt.tests.adapter.basic.files import seeds_base_csv, seeds_added_csv, seeds_newcolumns_csv  
from dbt.tests.adapter.basic.test_snapshot_check_cols import BaseSnapshotCheckCols  
  
# set the datatype of the name column in the 'added' seed so it  
# can hold the '_update' that's added  
schema_seed_added_yml = """  
version: 2  
seeds:  
  - name: added  
    config:  
      column_types:  
        name: varchar(64)  
"""  
  
class TestSnapshotCheckColsRedshift(BaseSnapshotCheckCols):  
    # Redshift defines the 'name' column such that it's not big enough  
    # to hold the '_update' added in the test.  
    @pytest.fixture(scope="class")  
    def models(self):  
        return {  
            "base.csv": seeds_base_csv,  
            "added.csv": seeds_added_csv,  
            "seeds.yml": schema_seed_added_yml,  
        }
```

As another example, the `dbt-bigquery` adapter asks users to "authorize" replacing a table with a view by supplying the `--full-refresh` flag. The reason: In the table materialization logic, a view by the same name must first be dropped; if the table query fails, the model will be missing.

Knowing this possibility, the "base" test case offers a `require_full_refresh` switch on the `test_config` fixture class. For BigQuery, we'll switch it on:

tests/functional/adapter/test\_basic.py

```
import pytest  
from dbt.tests.adapter.basic.test_base import BaseSimpleMaterializations  
  
class TestSimpleMaterializationsBigQuery(BaseSimpleMaterializations):  
    @pytest.fixture(scope="class")  
    def test_config(self):  
        # effect: add '--full-refresh' flag in requisite 'dbt run' step  
        return {"require_full_refresh": True}
```

It's always worth asking whether the required modifications represent gaps in perceived or expected dbt functionality. Are these simple implementation details, which any user of this database would understand? Are they limitations worth documenting?

If, on the other hand, they represent poor assumptions in the "basic" test cases, which fail to account for a common pattern in other types of databases-—please open an issue or PR in the `dbt-core` repository on GitHub.

### Running with multiple profiles[​](#running-with-multiple-profiles "Direct link to Running with multiple profiles")

Some databases support multiple connection methods, which map to actually different functionality behind the scenes. For instance, the `dbt-spark` adapter supports connections to Apache Spark clusters *and* Databricks runtimes, which supports additional functionality out of the box, enabled by the Delta file format.

tests/conftest.py

```
def pytest_addoption(parser):  
    parser.addoption("--profile", action="store", default="apache_spark", type=str)  
  
  
# Using @pytest.mark.skip_profile('apache_spark') uses the 'skip_by_profile_type'  
# autouse fixture below  
def pytest_configure(config):  
    config.addinivalue_line(  
        "markers",  
        "skip_profile(profile): skip test for the given profile",  
    )  
  
@pytest.fixture(scope="session")  
def dbt_profile_target(request):  
    profile_type = request.config.getoption("--profile")  
    elif profile_type == "databricks_sql_endpoint":  
        target = databricks_sql_endpoint_target()  
    elif profile_type == "apache_spark":  
        target = apache_spark_target()  
    else:  
        raise ValueError(f"Invalid profile type '{profile_type}'")  
    return target  
  
def apache_spark_target():  
    return {  
        "type": "spark",  
        "host": "localhost",  
        ...  
    }  
  
def databricks_sql_endpoint_target():  
    return {  
        "type": "spark",  
        "host": os.getenv("DBT_DATABRICKS_HOST_NAME"),  
        ...  
    }  
  
@pytest.fixture(autouse=True)  
def skip_by_profile_type(request):  
    profile_type = request.config.getoption("--profile")  
    if request.node.get_closest_marker("skip_profile"):  
        for skip_profile_type in request.node.get_closest_marker("skip_profile").args:  
            if skip_profile_type == profile_type:  
                pytest.skip("skipped on '{profile_type}' profile")
```

If there are tests that *shouldn't* run for a given profile:

tests/functional/adapter/basic.py

```
# Snapshots require access to the Delta file format, available on our Databricks connection,  
# so let's skip on Apache Spark  
@pytest.mark.skip_profile('apache_spark')  
class TestSnapshotCheckColsSpark(BaseSnapshotCheckCols):  
    @pytest.fixture(scope="class")  
    def project_config_update(self):  
        return {  
            "seeds": {  
                "+file_format": "delta",  
            },  
            "snapshots": {  
                "+file_format": "delta",  
            }  
        }
```

Finally:

```
python3 -m pytest tests/functional --profile apache_spark  
python3 -m pytest tests/functional --profile databricks_sql_endpoint
```

Document a new adapter[​](#document-a-new-adapter "Direct link to Document a new adapter")
------------------------------------------------------------------------------------------

If you've already built, and tested your adapter, it's time to document it so the dbt community will know that it exists and how to use it.

### Making your adapter available[​](#making-your-adapter-available "Direct link to Making your adapter available")

Many community members maintain their adapter plugins under open source licenses. If you're interested in doing this, we recommend:

* Hosting on a public git provider (for example, GitHub or Gitlab)
* Publishing to [PyPI](https://pypi.org/)
* Adding to the list of ["Supported Data Platforms"](/docs/supported-data-platforms#community-supported) (more info below)

### General Guidelines[​](#general-guidelines "Direct link to General Guidelines")

To best inform the dbt community of the new adapter, you should contribute to the dbt's open-source documentation site, which uses the [Docusaurus project](https://docusaurus.io/). This is the site you're currently on!

### Conventions[​](#conventions "Direct link to Conventions")

Each `.md` file you create needs a header as shown below. The document id will also need to be added to the config file: `website/sidebars.js`.

```
---  
title: "Documenting a new adapter"  
id: "documenting-a-new-adapter"  
---
```

### Single Source of Truth[​](#single-source-of-truth "Direct link to Single Source of Truth")

We ask our adapter maintainers to use the [docs.getdbt.com repo](https://github.com/dbt-labs/docs.getdbt.com) (i.e. this site) as the single-source-of-truth for documentation rather than having to maintain the same set of information in three different places. The adapter repo's `README.md` and the data platform's documentation pages should simply link to the corresponding page on this docs site. Keep reading for more information on what should and shouldn't be included on the dbt docs site.

### Assumed Knowledge[​](#assumed-knowledge "Direct link to Assumed Knowledge")

To simplify things, assume the reader of this documentation already knows how both dbt and your data platform works. There's already great material for how to learn dbt and the data platform out there. The documentation we're asking you to add should be what a user who is already profiecient in both dbt and your data platform would need to know in order to use both. Effectively that boils down to two things: how to connect, and how to configure.

### Topics and Pages to Cover[​](#topics-and-pages-to-cover "Direct link to Topics and Pages to Cover")

The following subjects need to be addressed across three pages of this docs site to have your data platform be listed on our documentation. After the corresponding pull request is merged, we ask that you link to these pages from your adapter repo's `README` as well as from your product documentation.

To contribute, all you will have to do make the changes listed in the table below.

| How To... | File to change within `/website/docs/` | Action | Info to include |
| --- | --- | --- | --- |
| Connect | `/docs/core/connect-data-platform/{MY-DATA-PLATFORM}-setup.md` | Create | Give all information needed to define a target in `~/.dbt/profiles.yml` and get `dbt debug` to connect to the database successfully. All possible configurations should be mentioned. |
| Configure | `reference/resource-configs/{MY-DATA-PLATFORM}-configs.md` | Create | What options and configuration specific to your data platform do users need to know? e.g. table distribution and indexing options, column\_quoting policy, which incremental strategies are supported |
| Discover and Install | `docs/supported-data-platforms.md` | Modify | Is it a vendor- or community- supported adapter? How to install Python adapter package? Ideally with pip and PyPI hosted package, but can also use `git+` link to GitHub Repo |
| Add link to sidebar | `website/sidebars.js` | Modify | Add the document id to the correct location in the sidebar menu |

For example say I want to document my new adapter: `dbt-ders`. For the "Connect" page, I will make a new Markdown file, `ders-setup.md` and add it to the `/website/docs/core/connect-data-platform/` directory.

### Example PRs to add new adapter documentation[​](#example-prs-to-add-new-adapter-documentation "Direct link to Example PRs to add new adapter documentation")

Below are some recent pull requests made by partners to document their data platform's adapter:

* [TiDB](https://github.com/dbt-labs/docs.getdbt.com/pull/1309)
* [SingleStore](https://github.com/dbt-labs/docs.getdbt.com/pull/1044)
* [Firebolt](https://github.com/dbt-labs/docs.getdbt.com/pull/941)

Note — Use the following re-usable component to auto-fill the frontmatter content on your new page:

```
import SetUpPages from '/snippets/_setup-pages-intro.md';  
  
<SetUpPages meta={frontMatter.meta} />
```

Promote a new adapter[​](#promote-a-new-adapter "Direct link to Promote a new adapter")
---------------------------------------------------------------------------------------

The most important thing here is recognizing that people are successful in the community when they join, first and foremost, to engage authentically.

What does authentic engagement look like? It’s challenging to define explicit rules. One good rule of thumb is to treat people with dignity and respect.

Contributors to the community should think of contribution *as the end itself,* not a means toward other business KPIs (leads, community members, etc.). [We are a mission-driven company.](https://www.getdbt.com/dbt-labs/values/) Some ways to know if you’re authentically engaging:

* Is an engagement’s *primary* purpose of sharing knowledge and resources or building brand engagement?
* Imagine you didn’t work at the org you do — can you imagine yourself still writing this?
* Is it written in formal / marketing language, or does it sound like you, the human?

### Who should join the dbt community slack?[​](#who-should-join-the-dbt-community-slack "Direct link to Who should join the dbt community slack?")

* People who have insight into what it means to do hands-on [analytics engineering](https://www.getdbt.com/analytics-engineering/) work
  The dbt Community Slack workspace is fundamentally a place for analytics practitioners to interact with each other — the closer the users are in the community to actual data/analytics engineering work, the more natural their engagement will be (leading to better outcomes for partners and the community).
* DevRel practitioners with strong focus
  DevRel practitioners often have a strong analytics background and a good understanding of the community. It’s essential to be sure they are focused on *contributing,* not on driving community metrics for partner org (such as signing people up for their slack or events). The metrics will rise naturally through authentic engagement.
* Founder and executives who are interested in directly engaging with the community
  This is either incredibly successful or not at all depending on the profile of the founder. Typically, this works best when the founder has a practitioner-level of technical understanding and is interested in joining not to promote, but to learn and hear from users.
* Software Engineers at partner products that are building and supporting integrations with either - Software Engineers at partner products that are building and supporting integrations with either dbt Core or the dbt Core
  or the dbt platform
  This is successful when the engineers are familiar with dbt as a product or at least have taken our training course. The Slack is often a place where end-user questions and feedback is initially shared, so it is recommended that someone technical from the team be present. There are also a handful of channels aimed at those building integrations, which tend to be a font of knowledge.

### Who might struggle in the dbt community[​](#who-might-struggle-in-the-dbt-community "Direct link to Who might struggle in the dbt community")

* People in marketing roles
  dbt Slack is not a marketing channel. Attempts to use it as such invariably fall flat and can even lead to people having a negative view of a product. This doesn’t mean that dbt can’t serve marketing objectives, but a long-term commitment to engagement is the only proven method to do this sustainably.
* People in product roles
  The dbt Community can be an invaluable source of feedback on a product. There are two primary ways this can happen — organically (community members proactively suggesting a new feature) and via direct calls for feedback and user research. Immediate calls for engagement must be done in your dedicated #tools channel. Direct calls should be used sparingly, as they can overwhelm more organic discussions and feedback.

### Who is the audience for an adapter release?[​](#who-is-the-audience-for-an-adapter-release "Direct link to Who is the audience for an adapter release?")

A new adapter is likely to drive huge community interest from several groups of people:

* People who are currently using the database that the adapter is supporting
* People who may be adopting the database in the near future.
* People who are interested in dbt development in general.

The database users will be your primary audience and the most helpful in achieving success. Engage them directly in the adapter’s dedicated Slack channel. If one does not exist already, reach out in #channel-requests, and we will get one made for you and include it in an announcement about new channels.

The final group is where non-slack community engagement becomes important. Twitter and LinkedIn are both great places to interact with a broad audience. A well-orchestrated adapter release can generate impactful and authentic engagement.

### How to message the initial rollout and follow-up content[​](#how-to-message-the-initial-rollout-and-follow-up-content "Direct link to How to message the initial rollout and follow-up content")

Tell a story that engages dbt users and the community. Highlight new use cases and functionality unlocked by the adapter in a way that will resonate with each segment.

* Existing users of your technology who are new to dbt

  + Provide a general overview of the value dbt will deliver to your users. This can lean on dbt's messaging and talking points which are laid out in the [dbt viewpoint.](/community/resources/viewpoint)
  + Give examples of a rollout that speaks to the overall value of dbt and your product.
* Users who are already familiar with dbt and the community

  + Consider unique use cases or advantages your adapter provide over existing adapters. Who will be excited for this?
  + Contribute to the dbt Community and ensure that dbt users on your adapter are well supported (tutorial content, packages, documentation, etc).
  + Example of a rollout that is compelling for those familiar with dbt: [Firebolt](https://www.linkedin.com/feed/update/urn:li:activity:6879090752459182080/)

### Tactically manage distribution of content about new or existing adapters[​](#tactically-manage-distribution-of-content-about-new-or-existing-adapters "Direct link to Tactically manage distribution of content about new or existing adapters")

There are tactical pieces on how and where to share that help ensure success.

* On slack:

  + #i-made-this channel — this channel has a policy against “marketing” and “content marketing” posts, but it should be successful if you write your content with the above guidelines in mind. Even with that, it’s important to post here sparingly.
  + Your own database / tool channel — this is where the people who have opted in to receive communications from you and always a great place to share things that are relevant to them.
* On social media:

  + Twitter
  + LinkedIn
  + Social media posts *from the author* or an individual connected to the project tend to have better engagement than posts from a company or organization account.
  + Ask your partner representative about:
    - Retweets and shares from the official dbt Labs accounts.
    - Flagging posts internally at dbt Labs to get individual employees to share.

#### Measuring engagement[​](#measuring-engagement "Direct link to Measuring engagement")

You don’t need 1000 people in a channel to succeed, but you need at least a few active participants who can make it feel lived in. If you’re comfortable working in public, this could be members of your team, or it can be a few people who you know that are highly engaged and would be interested in participating. Having even 2 or 3 regulars hanging out in a channel is all that’s needed for a successful start and is, in fact, much more impactful than 250 people that never post.

### How to announce a new adapter[​](#how-to-announce-a-new-adapter "Direct link to How to announce a new adapter")

We’d recommend *against* boilerplate announcements and encourage finding a unique voice. That being said, there are a couple of things that we’d want to include:

* A summary of the value prop of your database / technology for users who aren’t familiar.
* The personas that might be interested in this news.
* A description of what the adapter *is*. For example:
  > With the release of our new dbt adapter, you’ll be able to to use dbt to model and transform your data in [name-of-your-org]
* Particular or unique use cases or functionality unlocked by the adapter.
* Plans for future / ongoing support / development.
* The link to the documentation for using the adapter on the dbt Labs docs site.
* An announcement blog.

#### Announcing new release versions of existing adapters[​](#announcing-new-release-versions-of-existing-adapters "Direct link to Announcing new release versions of existing adapters")

This can vary substantially depending on the nature of the release but a good baseline is the types of release messages that [we put out in the #dbt-releases](https://getdbt.slack.com/archives/C37J8BQEL/p1651242161526509) channel.

![Full Release Post](/assets/images/0-full-release-notes-1cc8cb263cb178df48deda1f69875c99.png)

Breaking this down:

* Visually distinctive announcement - make it clear this is a release


  [![title](/img/adapter-guide/1-announcement.png?v=2 "title")](#)title
* Short written description of what is in the release


  [![description](/img/adapter-guide/2-short-description.png?v=2 "description")](#)description
* Links to additional resources


  [![more resources](/img/adapter-guide/3-additional-resources.png?v=2 "more resources")](#)more resources
* Implementation instructions:


  [![more installation](/img/adapter-guide/4-installation.png?v=2 "more installation")](#)more installation
* Contributor recognition (if applicable)


  [![thank yous](/img/adapter-guide/6-thank-contribs.png?v=2 "thank yous")](#)thank yous

Build a trusted adapter[​](#build-a-trusted-adapter "Direct link to Build a trusted adapter")
---------------------------------------------------------------------------------------------

The Trusted Adapter Program exists to allow adapter maintainers to demonstrate to the dbt community that your adapter is trusted to be used in production.

The very first data platform dbt supported was Redshift followed quickly by Postgres ([dbt-core#174](https://github.com/dbt-labs/dbt-core/pull/174)). In 2017, back when dbt Labs (née Fishtown Analytics) was still a data consultancy, we added support for Snowflake and BigQuery. We also turned dbt's database support into an adapter framework ([dbt-core#259](https://github.com/dbt-labs/dbt-core/pull/259/)), and a plugin system a few years later. For years, dbt Labs specialized in those four data platforms and became experts in them. However, the surface area of all possible databases, their respective nuances, and keeping them up-to-date and bug-free is a Herculean and/or Sisyphean task that couldn't be done by a single person or even a single team! Enter the dbt community which enables dbt Core to work on more than 30 different databases (32 as of Sep '22)!

Free and open-source tools for the data professional are increasingly abundant. This is by-and-large a *good thing*, however it requires due diligence that wasn't required in a paid-license, closed-source software world. Before taking a dependency on an open-source project is is important to determine the answer to the following questions:

1. Does it work?
2. Does it meet my team's specific use case?
3. Does anyone "own" the code, or is anyone liable for ensuring it works?
4. Do bugs get fixed quickly?
5. Does it stay up-to-date with new Core features?
6. Is the usage substantial enough to self-sustain?
7. What risks do I take on by taking a dependency on this library?

These are valid, important questions to answer—especially given that `dbt-core` itself only put out its first stable release (major version v1.0) in December 2021! Indeed, up until now, the majority of new user questions in database-specific channels are some form of:

* "How mature is `dbt-<ADAPTER>`? Any gotchas I should be aware of before I start exploring?"
* "has anyone here used `dbt-<ADAPTER>` for production models?"
* "I've been playing with `dbt-<ADAPTER>` -- I was able to install and run my initial experiments. I noticed that there are certain features mentioned on the documentation that are marked as 'not ok' or 'not tested'. What are the risks?
  I'd love to make a statement on my team to adopt dbt, but I'm pretty sure questions will be asked around the possible limitations of the adapter or if there are other companies out there using dbt with Oracle DB in production, etc."

There has been a tendency to trust the dbt Labs-maintained adapters over community- and vendor-supported adapters, but repo ownership is only one among many indicators of software quality. We aim to help our users feel well-informed as to the caliber of an adapter with a new program.

### What it means to be trusted[​](#what-it-means-to-be-trusted "Direct link to What it means to be trusted")

By opting into the below, you agree to this, and we take you at your word. dbt Labs reserves the right to remove an adapter from the trusted adapter list at any time, should any of the below guidelines not be met.

### Feature Completeness[​](#feature-completeness "Direct link to Feature Completeness")

To be considered for the Trusted Adapter Program, the adapter must cover the essential functionality of dbt Core given below, with best effort given to support the entire feature set.

Essential functionality includes (but is not limited to the following features):

* table, view, and seed materializations
* dbt tests

The adapter should have the required documentation for connecting and configuring the adapter. The dbt docs site should be the single source of truth for this information. These docs should be kept up-to-date.

Proceed to the "Document a new adapter" step for more information.

### Release cadence[​](#release-cadence "Direct link to Release cadence")

Keeping an adapter up-to-date with the latest features of dbt, as defined in [dbt-adapters](https://github.com/dbt-labs/dbt-adapters), is an integral part of being a trusted adapter. We encourage adapter maintainers to keep track of new dbt-adapter releases and support new features relevant to their platform, ensuring users have the best version of dbt.

Before [dbt Core version 1.8](/docs/dbt-versions/core-upgrade/upgrading-to-v1.8#new-dbt-core-adapter-installation-procedure), adapter versions needed to match the semantic versioning of dbt Core. After v1.8, this is no longer required. This means users can use an adapter on v1.8+ with a different version of dbt Core v1.8+. For example, a user could use dbt-core v1.9 with dbt-postgres v1.8.

### Community responsiveness[​](#community-responsiveness "Direct link to Community responsiveness")

On a best effort basis, active participation and engagement with the dbt Community across the following forums:

* Being responsive to feedback and supporting user enablement in dbt Community’s Slack workspace
* Responding with comments to issues raised in public dbt adapter code repository
* Merging in code contributions from community members as deemed appropriate

### Security Practices[​](#security-practices "Direct link to Security Practices")

Trusted adapters will not do any of the following:

* Output to logs or file either access credentials information to or data from the underlying data platform itself.
* Make API calls other than those expressly required for using dbt features (adapters may not add additional logging)
* Obfuscate code and/or functionality so as to avoid detection

Additionally, to avoid supply-chain attacks:

* Use an automated service to keep Python dependencies up-to-date (such as Dependabot or similar),
* Publish directly to PyPI from the dbt adapter code repository by using trusted CI/CD process (such as GitHub actions)
* Restrict admin access to both the respective code (GitHub) and package (PyPI) repositories
* Identify and mitigate security vulnerabilities by use of a static code analyzing tool (such as Snyk) as part of a CI/CD process

### Other considerations[​](#other-considerations "Direct link to Other considerations")

The adapter repository is:

* open-souce licensed,
* published to PyPI, and
* automatically tests the codebase against dbt Lab's provided adapter test suite

### How to get an adapter on the trusted list[​](#how-to-get-an-adapter-on-the-trusted-list "Direct link to How to get an adapter on the trusted list")

Open an issue on the [docs.getdbt.com GitHub repository](https://github.com/dbt-labs/docs.getdbt.com) using the "Add adapter to Trusted list" template. In addition to contact information, it will ask confirm that you agree to the following.

1. my adapter meet the guidelines given above
2. I will make best reasonable effort that this continues to be so
3. checkbox: I acknowledge that dbt Labs reserves the right to remove an adapter from the trusted adapter list at any time, should any of the above guidelines not be met.

The approval workflow is as follows:

1. create and populate the template-created issue
2. dbt Labs will respond as quickly as possible (maximally four weeks, though likely faster)
3. If approved, dbt Labs will create and merge a Pull request to formally add the adapter to the list.

### Getting help for my trusted adapter[​](#getting-help-for-my-trusted-adapter "Direct link to Getting help for my trusted adapter")

Ask your question in #adapter-ecosystem channel of the dbt community Slack.

Was this page helpful?
----------------------

YesNo

[Privacy policy](https://www.getdbt.com/cloud/privacy-policy)[Create a GitHub issue](https://github.com/dbt-labs/docs.getdbt.com/issues)

This site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

0

**Tags:**

* [Adapter creation](/tags/adapter-creation)

[Edit this page](https://github.com/dbt-labs/docs.getdbt.com/edit/current/website/docs/guides/adapter-creation.md)

Last updated on **Dec 4, 2025**

Get started

Start building with dbt.
------------------------

The free dbt VS Code extension is the best way to develop locally with the dbt Fusion Engine.

[Install free extension](https://marketplace.visualstudio.com/items?itemName=dbtLabsInc.dbt)
[Request your demo](https://www.getdbt.com/contact)

[![dbt Labs](/img/dbt-logo-light.svg?v=2)](/)

##### Resources

[VS Code Extension](/docs/about-dbt-extension)
[Resource Hub](https://www.getdbt.com/resources)
[dbt Learn](https://www.getdbt.com/dbt-learn)
[Certification](https://www.getdbt.com/dbt-certification)
[Developer Blog](/blog)

##### Community

[Join the Community](/community/join)
[Become a Contributor](/community/contribute)
[Open Source dbt Packages](https://hub.getdbt.com/)
[Community Forum](/community/forum)

##### Support

[Contact Support](/docs/dbt-support)
[Professional Services](https://www.getdbt.com/services)
[Find a Partner](https://www.getdbt.com/partner-directory)
[System Status](https://status.getdbt.com/)

##### Connect with Us

© 2025 dbt Labs, Inc. All Rights Reserved.

[Terms of Service](https://www.getdbt.com/terms-of-use/)
[Privacy Policy](https://www.getdbt.com/cloud/privacy-policy/)
[Security](https://www.getdbt.com/security/)
Cookie Settings