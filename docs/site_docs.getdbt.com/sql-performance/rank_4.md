# Source: https://docs.getdbt.com/best-practices/best-practice-workflows

Best practices for workflows | dbt Developer Hub

[Skip to main content](#__docusaurus_skipToContent_fallback)

[✨ Live virtual event - Smarter pipelines, 29% more efficient: How the dbt Fusion engine optimizes data work on December 3rd!](https://www.getdbt.com/resources/webinars/how-the-dbt-fusion-engine-optimizes-data-work)

[![dbt Logo](/img/dbt-logo.svg?v=2)![dbt Logo](/img/dbt-logo-light.svg?v=2)](/)

[Docs](#)

* [Product docs](/docs/introduction)
* [References](/reference/references-overview)
* [Best practices](/best-practices)
* [Developer blog](/blog)

[Guides](/guides)[APIs](/docs/dbt-cloud-apis/overview)

[Help](#)

* [Release notes](/docs/dbt-versions/dbt-cloud-release-notes)
* [FAQs](/docs/faqs)
* [Support and billing](/docs/dbt-support)
* [Fusion Diaries](https://github.com/dbt-labs/dbt-fusion/discussions/categories/announcements)
* [Courses](https://learn.getdbt.com)

[Community](#)

* [Join the dbt Community](/community/join)
* [Become a contributor](/community/contribute)
* [Community forum](/community/forum)
* [Events](/community/events)
* [Spotlight](/community/spotlight)

[Account](#)

* [Log in to dbt](https://cloud.getdbt.com/)
* [Create a free account](https://www.getdbt.com/signup)

[Install VS Code extension](https://marketplace.visualstudio.com/items?itemName=dbtLabsInc.dbt)

[v](#) 

* dbt platform (Latest)
* dbt Fusion engine
* Core v1.11 Beta
* Core v1.10 (Compatible)
* Core v1.9 (Extended)

Search

[![dbt Logo](/img/dbt-logo.svg?v=2)![dbt Logo](/img/dbt-logo-light.svg?v=2)](/)

* [Best practices](/best-practices)

  + [How we structure our dbt projects](/best-practices/how-we-structure/1-guide-overview)
  + [How we style our dbt projects](/best-practices/how-we-style/0-how-we-style-our-dbt-projects)
  + [How we build our metrics](/best-practices/how-we-build-our-metrics/semantic-layer-1-intro)
  + [How we build our dbt Mesh projects](/best-practices/how-we-mesh/mesh-1-intro)
  + [Materialization best practices](/best-practices/materializations/1-guide-overview)
  + [Don't nest your curlies](/best-practices/dont-nest-your-curlies)
  + [Clone incremental models as the first step of your CI job](/best-practices/clone-incremental-models)
  + [Writing custom generic data tests](/best-practices/writing-custom-generic-tests)
  + [Best practices for workflows](/best-practices/best-practice-workflows)
  + [Best practices for dbt and Unity Catalog](/best-practices/dbt-unity-catalog-best-practices)

* [Best practices](/best-practices)
* Best practices for workflows

Copy page

On this page

Best practices for workflows
============================

This page contains the collective wisdom of experienced users of dbt on how to best use it in your analytics work. Observing these best practices will help your analytics team work as effectively as possible, while implementing the pro-tips will add some polish to your dbt projects!

Best practice workflows[​](#best-practice-workflows "Direct link to Best practice workflows")
---------------------------------------------------------------------------------------------

### Version control your dbt project[​](#version-control-your-dbt-project "Direct link to Version control your dbt project")

All dbt projects should be managed in version control. Git branches should be created to manage development of new features and bug fixes. All code changes should be reviewed by a colleague (or yourself) in a Pull Request prior to merging into your production branch, such as `main`.

Git guide

We've codified our best practices in Git, in our [Git guide](https://github.com/dbt-labs/corp/blob/main/git-guide.md).

### Use separate development and production environments[​](#use-separate-development-and-production-environments "Direct link to Use separate development and production environments")

dbt makes it easy to maintain separate production and development environments through the use of targets within a profile. We recommend using a `dev` target when running dbt from your command line and only running against a `prod` target when running from a production deployment. You can read more [about managing environments here](/docs/environments-in-dbt).

### Use a style guide for your project[​](#use-a-style-guide-for-your-project "Direct link to Use a style guide for your project")

SQL styles, field naming conventions, and other rules for your dbt project should be codified, especially on projects where multiple dbt users are writing code.

Our style guide

We've made our [style guide](/best-practices/how-we-style/0-how-we-style-our-dbt-projects) public – these can act as a good starting point for your own style guide.

Best practices in dbt projects[​](#best-practices-in-dbt-projects "Direct link to Best practices in dbt projects")
------------------------------------------------------------------------------------------------------------------

### Use the ref function[​](#use-the-ref-function "Direct link to Use the ref function")

The [ref](/reference/dbt-jinja-functions/ref) function is what makes dbt so powerful! Using the `ref` function allows dbt to infer dependencies, ensuring that models are built in the correct order. It also ensures that your current model selects from upstream tables and views in the same environment that you're working in.
Always use the `ref` function when selecting from another model, rather than using the direct relation reference (e.g. `my_schema.my_table`).

### Limit references to raw data[​](#limit-references-to-raw-data "Direct link to Limit references to raw data")

Your dbt project will depend on raw data stored in your database. Since this data is normally loaded by third parties, the structure of it can change over time – tables and columns may be added, removed, or renamed. When this happens, it is easier to update models if raw data is only referenced in one place.

Using sources for raw data references

We recommend defining your raw data as [sources](/docs/build/sources), and selecting from the source rather than using the direct relation reference. Our dbt projects don't contain any direct relation references in any models.

### Rename and recast fields once[​](#rename-and-recast-fields-once "Direct link to Rename and recast fields once")

Raw data is generally stored in a source-conformed structure, that is, following the schema and naming conventions that the source defines. Not only will this structure differ between different sources, it is also likely to differ from the naming conventions you wish to use for analytics.

The first layer of transformations in a dbt project should:

* Select from only one source
* Rename fields and tables to fit the conventions you wish to use within your project, for example, ensuring all timestamps are named `<event>_at`. These conventions should be declared in your project coding conventions (see above).
* Recast fields into the correct data type, for example, changing dates into UTC and prices into dollar amounts.

All subsequent data models should be built on top of these models, reducing the amount of duplicated code.

What happened to base models?

Earlier versions of this documentation recommended implementing “base models” as the first layer of transformation, and gave advice on the SQL within these models. We realized that while the reasons behind this convention were valid, the specific advice around "base models" represented an opinion, so we moved it out of the official documentation.

You can instead find our opinions on [how we structure our dbt projects](/best-practices/how-we-structure/1-guide-overview).

### Break complex models up into smaller pieces[​](#break-complex-models-up-into-smaller-pieces "Direct link to Break complex models up into smaller pieces")

Complex models often include multiple Common Table Expressions (CTEs). In dbt, you can instead separate these CTEs into separate models that build on top of each other. It is often a good idea to break up complex models when:

* A CTE is duplicated across two models. Breaking the CTE into a separate model allows you to reference the model from any number of downstream models, reducing duplicated code.
* A CTE changes the grain of a the data it selects from. It's often useful to test any transformations that change the grain (as in, what one record represents) of your data. Breaking a CTE into a separate model allows you to test this transformation independently of a larger model.
* The SQL in a query contains many lines. Breaking CTEs into separate models can reduce the cognitive load when another dbt user (or your future self) is looking at the code.

### Group your models in directories[​](#group-your-models-in-directories "Direct link to Group your models in directories")

Within your `models/` directory, you can have any number of nested subdirectories. We leverage directories heavily, since using a nested structure within directories makes it easier to:

* Configure groups of models, by specifying configurations in your `dbt_project.yml` file.
* Run subsections of your DAG, by using the [model selection syntax](/reference/node-selection/syntax).
* Communicate modeling steps to collaborators
* Create conventions around the allowed upstream dependencies of a model, for example, "models in the `marts` directory can only select from other models in the `marts` directory, or from models in the `staging` directory".

### Add tests to your models[​](#add-tests-to-your-models "Direct link to Add tests to your models")

dbt provides a framework to test assumptions about the results generated by a model. Adding tests to a project helps provide assurance that both:

* your SQL is transforming data in the way you expect, and
* your source data contains the values you expect

Recommended tests

Our [style guide](https://github.com/dbt-labs/corp/blob/main/dbt_style_guide.md) recommends that at a minimum, every model should have a primary key that is tested to ensure it is unique, and not null.

### Consider the information architecture of your data warehouse[​](#consider-the-information-architecture-of-your-data-warehouse "Direct link to Consider the information architecture of your data warehouse")

When a user connects to a data warehouse via a SQL client, they often rely on the names of schemas, relations, and columns, to understand the data they are presented with. To improve the information architecture of a data warehouse, we:

* Use [custom schemas](/docs/build/custom-schemas) to separate relations into logical groupings, or hide intermediate models in a separate schema. Generally, these custom schemas align with the directories we use to group our models, and are configured from the `dbt_project.yml` file.
* Use prefixes in table names (for example, `stg_`, `fct_` and `dim_`) to indicate which relations should be queried by end users.

### Choose your materializations wisely[​](#choose-your-materializations-wisely "Direct link to Choose your materializations wisely")

[materialization](/docs/build/materializations) determine the way models are built through configuration. As a general rule:

* Views are faster to build, but slower to query compared to tables.
* Incremental models provide the same query performance as tables, are faster to build compared to the table materialization, however they introduce complexity into a project.

We often:

* Use views by default
* Use ephemeral models for lightweight transformations that shouldn't be exposed to end-users
* Use tables for models that are queried by BI tools
* Use tables for models that have multiple descendants
* Use incremental models when the build time for table models exceeds an acceptable threshold

Pro-tips for workflows[​](#pro-tips-for-workflows "Direct link to Pro-tips for workflows")
------------------------------------------------------------------------------------------

### Use the model selection syntax when running locally[​](#use-the-model-selection-syntax-when-running-locally "Direct link to Use the model selection syntax when running locally")

When developing, it often makes sense to only run the model you are actively working on and any downstream models. You can choose which models to run by using the [model selection syntax](/reference/node-selection/syntax).

### Run only modified models to test changes ("slim CI")[​](#run-only-modified-models-to-test-changes-slim-ci "Direct link to Run only modified models to test changes (\"slim CI\")")

To merge code changes with confidence, you want to know that those changes will not cause breakages elsewhere in your project. For that reason, we recommend running models and tests in a sandboxed environment, separated from your production data, as an automatic check in your git workflow. (If you use GitHub and dbt, read about [how to set up CI jobs](/docs/deploy/ci-jobs).

At the same time, it costs time (and money) to run and test all the models in your project. This inefficiency feels especially painful if your PR only proposes changes to a handful of models.

By comparing to artifacts from a previous production run, dbt can determine which models are modified and build them on top of of their unmodified parents.

```
dbt run -s state:modified+ --defer --state path/to/prod/artifacts  
dbt test -s state:modified+ --defer --state path/to/prod/artifacts
```

By comparing to artifacts from a previous production run, dbt can determine model and test result statuses.

* `result:fail`
* `result:error`
* `result:warn`
* `result:success`
* `result:skipped`
* `result:pass`

For smarter reruns, use the `result:<status>` selector instead of manually overriding dbt commands with the models in scope.

```
dbt run --select state:modified+ result:error+ --defer --state path/to/prod/artifacts
```

* Rerun all my erroneous models AND run changes I made concurrently that may relate to the erroneous models for downstream use

```
dbt build --select state:modified+ result:error+ --defer --state path/to/prod/artifacts
```

* Rerun and retest all my erroneous models AND run changes I made concurrently that may relate to the erroneous models for downstream use

```
dbt build --select state:modified+ result:error+ result:fail+ --defer --state path/to/prod/artifacts
```

* Rerun all my erroneous models AND all my failed tests
* Rerun all my erroneous models AND run changes I made concurrently that may relate to the erroneous models for downstream use
* There's a failed test that's unrelated to modified or error nodes(think: source test that needs to refresh a data load in order to pass)

```
dbt test --select result:fail --exclude <example test> --defer --state path/to/prod/artifacts
```

* Rerun all my failed tests and exclude tests that I know will still fail
* This can apply to updates in source data during the "EL" process that need to be rerun after they are refreshed

> Note: If you're using the `--state target/` flag, `result:error` and `result:fail` flags can only be selected concurrently(in the same command) if using the `dbt build` command. `dbt test` will overwrite the `run_results.json` from `dbt run` in a previous command invocation.

Only supported by v1.1 or newer.

By comparing to a `sources.json` artifact from a previous production run to a current `sources.json` artifact, dbt can determine which sources are fresher and run downstream models based on them.

```
# job 1  
dbt source freshness # must be run to get previous state
```

Test all my sources that are fresher than the previous run, and run and test all models downstream of them:

```
# job 2  
dbt source freshness # must be run again to compare current to previous state  
dbt build --select source_status:fresher+ --state path/to/prod/artifacts
```

To learn more, read the docs on [state](/reference/node-selection/syntax#about-node-selection).

Pro-tips for dbt Projects[​](#pro-tips-for-dbt-projects "Direct link to Pro-tips for dbt Projects")
---------------------------------------------------------------------------------------------------

### Limit the data processed when in development[​](#limit-the-data-processed-when-in-development "Direct link to Limit the data processed when in development")

In a development environment, faster run times allow you to iterate your code more quickly. We frequently speed up our runs by using a pattern that limits data based on the [target](/reference/dbt-jinja-functions/target) name:

```
select  
*  
from event_tracking.events  
{% if target.name == 'dev' %}  
where created_at >= dateadd('day', -3, current_date)  
{% endif %}
```

Another option is to use the [environment variable `DBT_CLOUD_INVOCATION_CONTEXT`](/docs/build/environment-variables#dbt-platform-context). This environment variable provides metadata about the execution context of dbt. The possible values are `prod`, `dev`, `staging`, and `ci`.

**Example usage**:

```
{% if env_var('DBT_CLOUD_INVOCATION_CONTEXT') != 'prod' %}
```

### Use grants to manage privileges on objects that dbt creates[​](#use-grants-to-manage-privileges-on-objects-that-dbt-creates "Direct link to Use grants to manage privileges on objects that dbt creates")

Use `grants` in [resource configs](/reference/resource-configs/grants) to ensure that permissions are applied to the objects created by dbt. By codifying these grant statements, you can version control and repeatably apply these permissions.

### Separate source-centric and business-centric transformations[​](#separate-source-centric-and-business-centric-transformations "Direct link to Separate source-centric and business-centric transformations")

When modeling data, we frequently find there are two stages:

1. Source-centric transformations to transform data from different sources into a consistent structure, for example, re-aliasing and recasting columns, or unioning, joining or deduplicating source data to ensure your model has the correct grain; and
2. Business-centric transformations that transform data into models that represent entities and processes relevant to your business, or implement business definitions in SQL.

We find it most useful to separate these two types of transformations into different models, to make the distinction between source-centric and business-centric logic clear.

### Managing whitespace generated by Jinja[​](#managing-whitespace-generated-by-jinja "Direct link to Managing whitespace generated by Jinja")

If you're using macros or other pieces of Jinja in your models, your compiled SQL (found in the `target/compiled` directory) may contain unwanted whitespace. Check out the [Jinja documentation](http://jinja.pocoo.org/docs/2.10/templates/#whitespace-control) to learn how to control generated whitespace.

Related docs[​](#related-docs "Direct link to Related docs")
------------------------------------------------------------

* [Updating our permissioning guidelines: grants as configs in dbt Core v1.2](/blog/configuring-grants)

Was this page helpful?
----------------------

YesNo

[Privacy policy](https://www.getdbt.com/cloud/privacy-policy)[Create a GitHub issue](https://github.com/dbt-labs/docs.getdbt.com/issues)

This site is protected by reCAPTCHA and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

0

[Edit this page](https://github.com/dbt-labs/docs.getdbt.com/edit/current/website/docs/best-practices/best-practice-workflows.md)

Last updated on **Nov 19, 2025**

[Previous

Writing custom generic data tests](/best-practices/writing-custom-generic-tests)[Next

Best practices for dbt and Unity Catalog](/best-practices/dbt-unity-catalog-best-practices)

* [Best practice workflows](#best-practice-workflows)
  + [Version control your dbt project](#version-control-your-dbt-project)
  + [Use separate development and production environments](#use-separate-development-and-production-environments)
  + [Use a style guide for your project](#use-a-style-guide-for-your-project)
* [Best practices in dbt projects](#best-practices-in-dbt-projects)
  + [Use the ref function](#use-the-ref-function)
  + [Limit references to raw data](#limit-references-to-raw-data)
  + [Rename and recast fields once](#rename-and-recast-fields-once)
  + [Break complex models up into smaller pieces](#break-complex-models-up-into-smaller-pieces)
  + [Group your models in directories](#group-your-models-in-directories)
  + [Add tests to your models](#add-tests-to-your-models)
  + [Consider the information architecture of your data warehouse](#consider-the-information-architecture-of-your-data-warehouse)
  + [Choose your materializations wisely](#choose-your-materializations-wisely)
* [Pro-tips for workflows](#pro-tips-for-workflows)
  + [Use the model selection syntax when running locally](#use-the-model-selection-syntax-when-running-locally)
  + [Run only modified models to test changes ("slim CI")](#run-only-modified-models-to-test-changes-slim-ci)
* [Pro-tips for dbt Projects](#pro-tips-for-dbt-projects)
  + [Limit the data processed when in development](#limit-the-data-processed-when-in-development)
  + [Use grants to manage privileges on objects that dbt creates](#use-grants-to-manage-privileges-on-objects-that-dbt-creates)
  + [Separate source-centric and business-centric transformations](#separate-source-centric-and-business-centric-transformations)
  + [Managing whitespace generated by Jinja](#managing-whitespace-generated-by-jinja)
* [Related docs](#related-docs)

[Edit this page](https://github.com/dbt-labs/docs.getdbt.com/edit/current/website/docs/best-practices/best-practice-workflows.md)

Get started

Start building with dbt.
------------------------

The free dbt VS Code extension is the best way to develop locally with the dbt Fusion Engine.

[Install free extension](https://marketplace.visualstudio.com/items?itemName=dbtLabsInc.dbt)
[Request your demo](https://www.getdbt.com/contact)

[![dbt Labs](/img/dbt-logo-light.svg?v=2)](/)

##### Resources

[VS Code Extension](/docs/about-dbt-extension)
[Resource Hub](https://www.getdbt.com/resources)
[dbt Learn](https://www.getdbt.com/dbt-learn)
[Certification](https://www.getdbt.com/dbt-certification)
[Developer Blog](/blog)

##### Community

[Join the Community](/community/join)
[Become a Contributor](/community/contribute)
[Open Source dbt Packages](https://hub.getdbt.com/)
[Community Forum](/community/forum)

##### Support

[Contact Support](/docs/dbt-support)
[Professional Services](https://www.getdbt.com/services)
[Find a Partner](https://www.getdbt.com/partner-directory)
[System Status](https://status.getdbt.com/)

##### Connect with Us

© 2025 dbt Labs, Inc. All Rights Reserved.

[Terms of Service](https://www.getdbt.com/terms-of-use/)
[Privacy Policy](https://www.getdbt.com/cloud/privacy-policy/)
[Security](https://www.getdbt.com/security/)
Cookie Settings




![dbt Labs](https://cdn.cookielaw.org/logos/4a2cde9e-5f84-44b2-bdbb-6a93354d1c72/e1199e19-1935-49fa-a4e2-bf7f9d08cee6/783d7c83-af8c-4032-901b-b3ec48982078/dbt-logo.png)

Privacy Preference Center
-------------------------

When you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.
  
[More information](https://www.getdbt.com/cloud/privacy-policy/)

Allow All

### Manage Consent Preferences

#### Strictly Necessary Cookies

Always Active

Strictly necessary cookies are necessary for the site to function properly and cannot be switched off in our systems. These cookies are usually only set in response to actions made by you that amount to a request for services, such as setting your privacy preferences, logging in, or filling in forms. You can set your browser to block or alert you about these cookies, but blocking these cookies will prevent the site from functioning properly. These cookies typically do not store personal data.

#### Performance Cookies

Always Active

Performance cookies allow us to count visits and traffic sources so we can measure and improve the performance of our sites. These cookies help us understand how our sites are being used, such as which sites are the most and least popular and how people navigate around the sites. The information collected in these cookies are aggregated, meaning that the do not relate to you personally. Opting out of these cookies will prevent us from knowing when you have visited our site and will prevent us from monitoring site performance. In some cases, these cookies may be sent to our third party service providers to help us manage these analytics.

#### Targeting Cookies

Always Active

Targeting cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant advertisements on other sites. These cookies do not store directly personal information, but are based on uniquely identifying your browser and device. If you do not allow these cookies, you will experience less targeted advertising.

#### Functional Cookies

Always Active

Functional cookies enable our sites to provide enhanced functionality and personalization. They may be set by us or by third party service providers whose services we have added to our sites. If you reject these cookies, then some or all of these services may not function properly.

Back Button

### Cookie List

Search Icon

Filter Icon

Clear

checkbox label label

Apply Cancel

Consent Leg.Interest

checkbox label label

checkbox label label

checkbox label label

Confirm My Choices

[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg "Powered by OneTrust Opens in a new Tab")](https://www.onetrust.com/products/cookie-consent/)