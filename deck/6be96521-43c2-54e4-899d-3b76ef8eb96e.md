---
tags:
- card_type/factual
- development/cli-commands
- development/jinja-macros
- development/seeds
citations:
- cleaned_docs/seeds/rank_6.md
guid: bb9a6c1a31
source: llm
uuid: 6be96521-43c2-54e4-899d-3b76ef8eb96e
claim_meta:
  verdict: SUPPORTED
  explanation: "The Answer correctly identifies that dbt seed is discouraged for large datasets due to performance issues and version control concerns. The Reference Text explicitly states that dbt seed is not performant for large files and should not be used for raw data, implying version control problems, which supports the Answer's claims."
  citation:
    quote: "Loading CSVs using dbt's seed functionality is not performant for large files. Consider using a different tool to load these CSVs into your data warehouse."
    is_quote_valid: true
---

<front>

Why is using `dbt seed` generally discouraged for datasets larger than a few megabytes?

</front>

---

<back>

The `dbt seed` command is not optimized for performance and inserts data row-by-row or in small batches, which is extremely slow for large files. Furthermore, storing large CSVs in the dbt project causes git repository bloat, making version control operations slow and difficult to manage.

</back>
